# 随机森林

集成分为boosting和bagging。

### bagging

<img src="https://gitee.com/karlhan/picgo/raw/master/img//1042406-20161204200000787-1988863729.png" alt="img" style="zoom: 67%;" />

Bagging的弱学习器之间的确没有boosting那样的联系。它的特点在“随机采样”。

GBDT的随机采样是不放回采样，Bagging的子采样是放回采样。

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$1/m$。不被采集到的概率为$1−1/m$。如果m次采样都没有被采集中的概率是$(1−1/m)^m$。当$m→∞$时，$(1−1/m)^m→1/e≃0.368$也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。

对于这部分大约36.8%的没有被采样到的数据，我们常常称之为袋外数据(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。GBDT只使用CART回归或分类树

bagging的集合策略也比较简单，对于分类问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。

由于Bagging算法每次都进行采样来训练模型，因此泛化能力很强，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的偏倚会大一些。

### Bagging算法流程

输入为样本集$D={(x,y1),(x2,y2),...(x_m,y_m)}$，弱学习器算法, 弱分类器迭代次数T。

输出为最终的强分类器$f(x)$

1）对于t=1,2...,T:

　　a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_t$

　　b)用采样集$D_t$训练第t个弱学习器$G_t(x)$

2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

### 随机森林算法（RF）

对于bagging进行改进。

RF使用了CART决策树作为弱学习器，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过随机选择节点上的一部分样本特征，这个数字小于n，假设为$n_{sub}$，然后在这些随机选择的$n_{sub}$个样本特征中，选择一个最优的特征来做决策树的左右子树划分。这样进一步增强了模型的泛化能力。　　　　

如果$n_{sub}=n$，则此时RF的CART决策树和普通的CART决策树没有区别。$n_{sub}$越小，则模型约健壮，当然此时对于训练集的拟合程度会变差。也就是说$n_{sub}$越小，模型的方差会减小，但是偏倚会增大。在实际案例中，一般会通过交叉验证调参获取一个合适的$n_{sub}$的值。

　　　　除了上面两点，RF和普通的bagging算法没有什么不同， 下面简单总结下RF的算法。

　　　　输入为样本集$D={(x,y1),(x2,y2),...(x_m,y_m)}$，弱分类器迭代次数T。

　　　　输出为最终的强分类器f(x)

1）对于t=1,2...,T:

　a)对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_t$

　b)用采样集$D_t$训练第t个决策树模型$G_t(x)$，在训练决策树模型的节点的时候， 在节点上所有的样本特征中选择一部分样本特征， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分

2) 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。

### 随机森林的推广



















