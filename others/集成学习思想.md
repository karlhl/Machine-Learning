# 集成学习

基本理论：

首先集成分为同质学习器，异质学习器。每个个体学习期称为基学习器、组件学习器、弱学习器。

假设简单的少数服从多数的集成学习器，随着个体学习期的数目T增大，集成的错误率指数下降，最后趋向于0.

集成方法分为两大类：

- Boosting:个体学习器之间存在强依赖关系，必须串行生成的序列化方法。
- Bagging：不存在强依赖关系，并行化方法。随机森林



如果单个分类器表现很好，为什么不使用多个分类器？

通过集成学习可以提高整体的泛化能力，但是这种提高是有条件的：

（1）分类器之间应该有差异性；

（2）每个分类器的精度必须大于0.5；

如果使用的分类器没有差异，那么集成起来的分类结果是没有变化的。如下图所示，分类器的精度p<0.5，随着集成规模的增加，分类精度不断下降；如果精度大于p>0.5，那么最终分类精度可以趋向于1。

![img](https://gitee.com/karlhan/picgo/raw/master/img//20150913131957469)

如何获取多个独立的分类器？

我们首先想到的是用不同的机器学习算法训练模型，比如决策树、k-NN、神经网络、梯度下降、贝叶斯等等，但是这些分类器并不是独立的，它们会犯相同的错误，因为许多分类器是线性模型，它们最终的投票（voting）不会改进模型的预测结果。

既然不同的分类器不适用，那么可以尝试将数据分成几部分，每个部分的数据训练一个模型。这样做的优点是不容易出现过拟合，缺点是数据量不足导致训练出来的模型泛化能力较差。

下面介绍两种比较实用的方法Bagging和Boosting。

### Bagging算法(套袋法)

代表：AdaBoost

Bagging（Bootstrap Aggregating）是通过组合随机生成的训练集而改进分类的集成算法。Bagging每次训练数据时只使用训练集中的某个子集作为当前训练集（有放回随机抽样），每一个训练样本在某个训练集中可以多次或不出现，经过T次训练后，可得到T个不同的分类器。对一个测试样例进行分类时，分别调用这T个分类器，得到T个分类结果。最后把这T个分类结果中出现次数多的类赋予测试样例。这种抽样的方法叫做[bootstrap](https://en.wikipedia.org/wiki/Bootstrapping_(statistics))，就是利用有限的样本资料经由多次重复抽样，重新建立起足以代表原始样本分布之新样本。

1. 从原始样本集中使用Bootstraping方法**随机抽取n个训练样本，共进行k轮抽取，得到k个训练集。**（k个训练集之间相互独立，**元素可以有重复**）
2. **对于k个训练集，我们训练k个模型**（这k个模型可以根据具体问题而定，比如决策树，knn等）
3. 对于分类问题：由投票表决产生分类结果；对于回归问题：由k个模型预测结果的均值作为最后预测结果。***\*（所有模型的重要性相同）\****

Bagging算法基本步骤：

![img](http://img.blog.csdn.net/20150414212456280?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvYXNwaXJpbnZhZ3JhbnQ=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

 

因为是随机抽样，那这样的抽样只有63%的样本是原始数据集的。

![img](http://img.blog.csdn.net/20150913134119347?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

Bagging的优势在于当原始样本中有噪声数据时，通过bagging抽样，那么就有1/3的噪声样本不会被训练。对于受噪声影响的分类器，bagging对模型是有帮助的。所以说bagging可以降低模型的方差，不容易受噪声的影响，广泛应用在不稳定的模型，或者倾向于过拟合的模型。

### Boosting算法（提升法）

1. 对于训练集中的每个样本建立权值wi，**表示对每个样本的关注度**。当某个样本被误分类的概率很高时，需要加大对该样本的**权值**。
2. 进行**迭代的过程**中，***\*每一步迭代都是一个弱分类器\****。我们需要用***\*某种策略将其组合\****，作为最终模型。（例如AdaBoost给每个弱分类器一个权值，将其线性组合最为最终分类器。**误差越小的弱分类器，权值越大**）
3. 开始样本 和 模型(最好是弱模型)权重相同。经过第一个分类后。错误的样本会被增加权重。被后续的分类器重点关注。这样，后续模型就会对这些错误数据分类准确度好。然后依次下去。最后投票的时候，也有权重。错误率越高，权重越低。

Boosting算法指将弱学习算法组合成强学习算法，它的思想起源于Valiant提出的PAC(Probably Approximately Correct)学习模型。

基本思想：不同的训练集是通过调整每个样本对应的权重实现的，不同的权重对应不同的样本分布，而这个权重为分类器不断增加对错分样本的重视程度。

1. 首先赋予每个训练样本相同的初始化权重，在此训练样本分布下训练出一个弱分类器；

2. 利用该弱分类器更新每个样本的权重，分类错误的样本认为是分类困难样本，权重增加，反之权重降低，得到一个新的样本分布；

3. 在新的样本分布下，在训练一个新的弱分类器，并且更新样本权重，重复以上过程T次，得到T个弱分类器。

通过改变样本分布，使得分类器聚集在那些很难分的样本上，对那些容易错分的数据加强学习，增加错分数据的权重。这样错分的数据再下一轮的迭代就有更大的作用（对错分数据进行惩罚）。对于这些权重，一方面可以使用它们作为抽样分布，进行对数据的抽样；另一方面，可以使用权值学习有利于高权重样本的分类器，把一个弱分类器提升为一个强分类器。

弱分类器

![img](https://gitee.com/karlhan/picgo/raw/master/img//20150412195048739)

Boosting处理过程

![img](https://gitee.com/karlhan/picgo/raw/master/img//20150412195048739)

Boosting算法通过权重投票的方式将T个弱分类器组合成一个强分类器。只要弱分类器的分类精度高于50%，将其组合到强分类器里，会逐渐降低强分类器的分类误差。

由于Boosting将注意力集中在难分的样本上，使得它对训练样本的噪声非常敏感，主要任务集中在噪声样本上，从而影响最终的分类性能。

对于Boosting来说，有两个问题需要回答：一是在每一轮如何如何改变训练数据的概率分布；二是如何将多个弱分类器组合成一个强分类器。而且存在一个重大的缺陷：该分类算法要求预先知道弱分类器识别准确率的下限。



Boosting与Bagging的不同之处：

1、Bagging的训练集是随机的，以独立同分布选取的训练样本子集训练弱分类器，而Boosting训练集的选择不是独立的，每一次选择的训练集都依赖于上一次学习的结果，根据错误率取样，因此Boosting的分类精度在大多数数据集中要优于Bagging，但是在有些数据集中，由于过拟合的原因，Boosting的精度会退化。
2、Bagging的每个预测函数(即弱假设)没有权重，而Boosting根据每一次训练的训练误差得到该次预测函数的权重；
3、Bagging的各个预测函数可以并行生成，而Boosting的只能顺序生成。

4、[Bagging是减少variance，而Boosting是减少bias。](https://www.zhihu.com/question/26760839)



1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT

### stacking

Stacking技术是一种非常精美而复杂的，对模型集成的策略。常常用于各大数据挖掘竞赛中。有的时候数据挖掘比赛就会有这样的情况，一个非常厉害的单模型得出的结果，最终在几个模型融合结果的比拼下，败下阵来。Stacking实现了一种“三个臭皮匠顶一个诸葛亮”的效果。

利用多个算法，从不同角度去观测数据集，在不同的数据空间角度和数据结构角度（盲人站在不同的位置）来观测数据（摸象），然后再依据它自己的观测（摸象的结果），结合自己的算法原理（盲人自己的理解），来建立一个模型，在新的数据集上再进行预测，从而达到战胜诸葛亮的效果。

1. 在训练集上采用算法A、B、C等训练出一系列基学习器。
2. 用这些基学习器的输出结果组成新的训练集，在其上训练一个元学习器（meta-classifier，通常为线性模型LR），用于组织利用基学习器的答案，也就是将基层模型的答案作为输入，让元学习器学习组织给基层模型的答案分配权重

![img](https://gitee.com/karlhan/picgo/raw/master/img//20171117094740886)

XGB模型，把train分train1~train5,共5份，用其中4份预测剩下的那份,同时预测test数据，这样的过程做5次,生成5份train（原train样本数/5）数据和5份test数据。然后把5份预测的train数据纵向叠起来，把test预测的结果做平均。

RF模型和XGB模型一样，再来一次。这样就生成了2份train数据和2份test数据（XGB重新表达的数据和RF重新表达的数据），然后用LR模型，进一步做融合，得到最终的预测结果。























参考资料：

1. [集成学习算法总结----Boosting和Bagging](https://www.cnblogs.com/sddai/p/7647731.html)
2. [集成学习思想总结](https://blog.csdn.net/changdejie/article/details/86707197)

















































