# 深度学习——参数初始化方法

初始化必要条件：

各层激活值不会出现饱和现象

各层激活值不为0

### 结论

随机初始化：适用：tanh

Xavier: 适用：前馈网络,   不适用：sigmoid函数和relu函数

He初始化 ： 适用：ReLU

### 标准初始化

权重矩阵初始化为下种形式：

<img src="https://gitee.com/karlhan/picgo/raw/master/img//image-20200908141900813.png" alt="image-20200908141900813" style="zoom:60%;" />

U是均匀分布，所以根据概率论得到方差：

<img src="https://gitee.com/karlhan/picgo/raw/master/img//image-20200908141958592.png" alt="image-20200908141958592" style="zoom:67%;" />

可以看出标准初始化方法得到一个非常好的特性：隐层的状态的均值为0，方差为常量1/3，和网络的层数无关，这意味着对于sigmoid函数来说，自变量落在有梯度的范围内。
但是因为sigmoid激活值都是大于0的，会导致下一层的输入不满足E(⋅)==0。其实标准初始化也只适用于满足下面将要提到的Glorot假设的激活函数，比如tanh。



### Xavier初始化

“Xavier”初始化方法是一种很有效的神经网络初始化方法，方法来源于2010年的一篇论文[《Understanding the difficulty of training deep feedforward neural networks》](http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2010_GlorotB10.pdf)

而Glorot认为：优秀的初始化应该使得各层的激活值和状态梯度的方差在传播过程中的方差保持一致，称为Glorot条件：

<img src="https://gitee.com/karlhan/picgo/raw/master/img//image-20200908142433680.png" alt="image-20200908142433680" style="zoom: 67%;" />

为了使得网络中信息更好的流动，每一层输出的方差应该尽量相等。基于这个目标，现在我们就去推导一下：每一层的权重应该满足哪种条件。

文章先假设的是线性激活函数，而且满足0点处导数为1，即
![这里写图片描述](https://gitee.com/karlhan/picgo/raw/master/img//20160507180808647)

现在我们先来分析一层卷积：
![这里写图片描述](https://gitee.com/karlhan/picgo/raw/master/img//20160507180808647)
其中ni表示输入个数。

根据概率统计知识我们有下面的方差公式：
![这里写图片描述](https://gitee.com/karlhan/picgo/raw/master/img//20160507180808647)

特别的，当我们假设输入和权重都是0均值时（目前有了BN之后，这一点也较容易满足），上式可以简化为：
![这里写图片描述](https://gitee.com/karlhan/picgo/raw/master/img//20160507180808647)

进一步假设输入x和权重w独立同分布，则有：
![这里写图片描述](https://gitee.com/karlhan/picgo/raw/master/img//20160507180808647)

于是，为了保证**输入与输出方差一致**，则应该有：
![这里写图片描述](https://gitee.com/karlhan/picgo/raw/master/img//20160507180808647)

对于一个多层的网络，某一层的方差可以用累积的形式表达：
![这里写图片描述](https://img-blog.csdn.net/20160507182047578)

特别的，反向传播计算梯度时同样具有类似的形式：
![这里写图片描述](https://img-blog.csdn.net/20160507182201262)

综上，为了保证前向传播和反向传播时每一层的方差一致，应满足：

![这里写图片描述](https://img-blog.csdn.net/20160507182402488)

但是，实际当中输入与输出的个数往往不相等，于是为了均衡考量，**最终我们的权重方差应满足**：

<img src="https://img-blog.csdn.net/20160507182552982" alt="这里写图片描述" style="zoom:50%;" />
学过概率统计的都知道 [a,b] 间的均匀分布的方差为：
<img src="https://img-blog.csdn.net/20160507182843806" alt="这里写图片描述" style="zoom:67%;" />

因此，**Xavier**初始化的实现就是下面的均匀分布：

<img src="https://img-blog.csdn.net/20160507183053496" alt="这里写图片描述" style="zoom:50%;" />

caffe的Xavier实现有三种选择

**（1）** 默认情况，方差只考虑输入个数：
![这里写图片描述](https://img-blog.csdn.net/20160507183438673)

**（2）** FillerParameter_VarianceNorm_FAN_OUT，方差只考虑输出个数：
![这里写图片描述](https://img-blog.csdn.net/20160507183630098)

**（3）** FillerParameter_VarianceNorm_AVERAGE，方差同时考虑输入和输出个数：
![这里写图片描述](https://img-blog.csdn.net/20160507183816880)

### He







参考:

1. [深度学习——Xavier初始化方法](https://blog.csdn.net/shuzfan/article/details/51338178)
2. [深度学习之参数初始化——Xavier初始化](https://blog.csdn.net/weixin_35479108/article/details/90694800)